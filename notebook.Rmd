---
title: "Intro to Bayesian Modeling in R"
output: html_notebook
---
<br/>

# First install JAGS (Just Another Gibbs Sampler)
Download JAGS [here](https://sourceforge.net/projects/mcmc-jags/)

## A handy function for sharing scripts that use packages
This function will require a package, or install and load it if it is not already.
```{r}
using <- function(...) {

    libs <- unlist(list(...))

    req <- unlist(lapply(libs, require, character.only=TRUE, quietly=TRUE)) 

    need <- libs[req==FALSE]

    if(length(need) > 0){ 

        install.packages(need)

        lapply(need, require, character.only=TRUE, quietly=TRUE)

    }

}  

```

<br/>  

Load the packages we'll need:
```{r,results='hide'}

 using("runjags", "rjags", "tidybayes", "tidyverse", "magrittr")


```
<br/>

## **Background**
**Statistics**: making inferences about an uncertain world using samples of observations (data)

The bedrock of statistics is probability theory. Much early work in probability and statistics were done by Bayesians. In the late 19$^{th}$ and early 20$^{th}$ centuries, statistics was taken over by Fisher and the frequentists. The exponential growth in human computing power and advent of sampling algorithms like MCMC during the 20$^{th}$ made previously intractible problems solvable. Today the debate between these philosophies of statistics is over among mathemeticians and statisticians.

    
The difference between Bayesian and Classical (a.k.a. frequentist) inference is subtle.  

  
### **Freqentist:** 
* Data are random.  
* Parameters ($\theta$) are fixed, but uncertain.

Estimate $\theta$ as the value that maximizes $P(data|\theta)$ (maximum likelihood)

Parameter estimates interpreted in terms of long-run frequency of outcomes in hypothetical repeated experiments.  
  
A 95% _confidence_ interval is interpreted as:  
"if the experiment were repeated a large number of times, the 'true' $\theta$ would fall within this range 95% of the time.
     
### **Bayesian:** 
* Data are fixed. 
* Parameters ($\theta$) are random

Estimate $P(\theta|data)$ using Bayes' rule:
$$ P(\theta|data)\propto \frac{P(data|\theta)P(\theta)}{P(data)}$$


MCMC is the the engine that gets us our answer.  

Parameter estimates are a probability distribution, interpretation is thus very straightforward. Posterior estimates can be interpreted the way most people probably _think_ about traditional confidence intervals.  

A 95% credible interveral would be interpreted as:
"Given these data and this model, there is a 95% chance $\theta$ lies in this range".  

## Statistics now

Whichever school of thought  you choose, inference is moving away from tests of 'significance' and $p$ values, focusing more on making inference about parameter estimates and their uncertainty (i.e., estimates and CI's).  

A very recent [paper](https://www.nature.com/articles/d41586-019-00857-9) in Nature calls for the retirement of $p$ values and suggests rebranding confidence intervals and credible intervals as 'compatibility intervals'. Both 'confidence' and 'credible' are somewhat loaded terms. Inferences made from data and models are only _credible_ (or you can only have _confidence_ in the inferences) if the assumptions you've made in collecting the data and building the model are correct (remembering all models wrong, some useful). Instead, we should consider point and interval estimates from a model to be _compatible_ with the data and assumed model structure.  
  

 
![Yoda's image &copy; Disney](yoda.jpg)  

## Why go Bayesian?  
 
**Reasons for:**  

* Straightforward interpretation of results (direct probability statements about parameters).
* Propagation of uncertainty (compatibility intervals on everything for _FREE_!!).  
* Tests for differences very straightforward, allow you to calculate probability of a difference.
* Approachable modeling notation, which provides a 'common tongue' for modelers to share ideas transparently.

     + JAGS models are written almost entirely in the mathematical notation you would to describe a model in a write-up.
     + Stan is too, but its syntax is a bit more complicated. I've found learning the basics of Bayesian Inference to be easier with JAGS models.

**Caveats**  

* There is a bit more overhead in terms of model setup and obtaining results:  
     + Determining appropriate priors
     + Tuning the model to reach convergence
     + Computation time



## Example 1: Estimating the probability of occurrence a binary outcome.
<br/>

Lets say someone wants to know if a coin they have is fair, or if it is a trick coin from a magic store.
Now before flipping the coin, this person has no idea whether this coin is fair or not, because the friend that gave it to them is known for practical jokes.

Our investigator wants to estimate something about this coin, namely what is the probability of getting a heads on a flip of the coin ($\theta$)?

To use Bayes rule to estimate $\theta$ we'll need some data, a likelihood for those data, and a prior distribution for our parameters:

$$ P(parameters|data)\propto \frac{likelihood \times prior}{P(data)}$$
We don't need to worry about $P(data)$, it is a constant (remember in Bayes' world the data are fixed), so is integrated out.

Because we are omniscient in this story, we know that the coin is a fair coin.

```{r }

# For reproducibility
set.seed(1141)

# Simulate 100 tosses of a fair coin
y <- rbinom(n=100 , size=1, p=0.5)

```

Now we have some data ($y$), next we need a model for $P(data|\theta)$ (the probability of observing these data, given the parameters). This is the 'likelihood'of the data. Here is where we choose a probability distribution that represents how observations $y$ came to be.

In this case, a binomial distribution is a good model of a binary outcome. So we can write the likelihood ($P(data|\theta)$) of the $data$ as: $$y\sim binomial(p, N)$$ , where $p$ is the probability of getting a heads, and $N$ is the number of coin tosses. We'll use the $bernoulli$ distribution in our model, it's just a binomial with N=1.

Now how should we characterize our prior beliefs $P(\theta)$?  
Lets say our investigator has no belief either way whether this coin is fair or not. Because our parameter $\theta$ is a probability, it has to be between 0 and 1.  But to reflect the investigators lack of knowledge, perhaps they believe _a priori_ that $\theta$ is equally likely to be anywhere between 0 and 1. They might then choose a uniform distribution between 0 and 1 as  $P(\theta)$):
$$\theta\sim uniform(0,1)$$  
Now we have everything we need to define our model.  
  
## Some notes on JAGS  
  
* JAGS takes models as a text string.  
* JAGS is a declarative language, not procedural (like R: do this, then that, etc.). 
* Declarative means describing what to do, not how to do it. It doesnt matter what order you specify your model.  
     + Another example of declarative is SQL (pick a dialect)  
*  Can edit model text in another text editor for help tracking parens and brackets etc.  

```{r}
# JAGS model of our coin-toss experiment
 mod <- 
      "model{

           # Likelihood of the data: P(data|theta)
           for (i in 1:n_tosses){
                y[i] ~ dbern(theta)
           }

           # Prior on theta: P(theta)
           theta ~ dunif(0, 1)
      
     }"
```

We'll use rjags and coda for this example.
```{r, results='hide'}
# Prepare data for JAGS:
data <- list(y=y, 
             n_tosses=length(y))
  
# Initialize our JAGS model
jm <- jags.model(textConnection(mod), data=data, n.chains=3)

# Burn-in
update(jm, 1000)

# Run the mcmc sampler
samps <- coda.samples(jm, variable.names="theta", n.iter=100000, thin=4)

```

**Trace plots and posterior distribution of $\theta$**

```{r}
plot(samps)

```
<br/>  

# Linear regression with the Iris data

```{r}
mod <-"
model{

# For each observation in 1:n  
  for (i in 1:n){

  # Likelihood of the data

     pw[i] ~ dnorm(mu[i], tau)


  # Regression equation

     mu[i] <-  B0 + B1*pl[i]
  
  # Calculate residuals 'by hand'
     resid[i] <-  mu[i] - pw[i]
  } 


  # Priors
  # These are 'vague' priors. 
  #  Should give same answer as a MLE solution

        # Intercept
        B0 ~ dnorm(0, 1/1000)

        # Slope
        B1 ~ dnorm(0, 1/1000)


    # Prior on standard deviation of residuals
        resid.sd ~ dgamma(0.001, 0.001)

    # Transform sd to precision (JAGS uses precision instead of variance)
        tau <- 1 / resid.sd^2

}
"

```
  
We'll use runjags with parallel chains for this example.  
  
```{r}

# Prepare the iris data in a list
data <- list(pw=iris$Petal.Width,
             pl=iris$Petal.Length,
             n=nrow(iris))

# Run the model
results <- run.jags(model=mod, 
              data=data, 
              n.chains=3,
              adapt=1000,
              burnin=5000,
              monitor=c("B0", "B1", "resid.sd"),
              sample=10000,
              thin=5,
              method="rjparallel")
```

```{r}

# Check out summary of model run
summary(results)
```

```{r}

# Check convergence
results$mcmc %>% plot()
```

```{r}
# Look for autocorrelation in chains
results$mcmc %>% autocorr.plot()
```

```{r}
# MLE fit
m <- lm(Petal.Width ~ Petal.Length, data=iris) %>% summary

# Do we get the same answer?
bayes <- summary(results$mcmc)[[1]][,1:2]

MLE <- rbind(Coefs=m$coefficients[,1:2], Resid.sd=c(m$sigma, NA))

print(list(Bayesian=bayes, Classical=MLE))
```
  
  
<br/>

# Session Info (for reproducibility)  
  
```{r,echo=FALSE}
print(sessionInfo())
```


