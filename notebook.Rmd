---
title: "Intro to Bayesian Modeling in R"
output:
  html_document:
    df_print: paged
---
<br/>

# First install JAGS (Just Another Gibbs Sampler)
Download JAGS [here](https://sourceforge.net/projects/mcmc-jags/)

## A handy function for sharing scripts that use packages
This function will require a package, or install and load it if it is not already.
```{r}
using <- function(...) {

    libs <- unlist(list(...))

    req <- unlist(lapply(libs, require, character.only=TRUE, quietly=TRUE)) 

    need <- libs[req==FALSE]

    if(length(need) > 0){ 

        install.packages(need)

        lapply(need, require, character.only=TRUE, quietly=TRUE)

    }

}  

```

<br/>  

**Load the packages we'll need:**
```{r load packages,results='hide',warning=FALSE,message=FALSE}

 using("runjags", "rjags", "tidybayes", "tidyverse", "magrittr")


```
**Setup STAN**
```{r,warning=FALSE,message=FALSE,results='hide'}
require(rstan)||install.packages("rstan",dependencies=TRUE);library(rstan)

# Set cores for parallel processing
options(mc.cores=parallel::detectCores()-1)

# Check for a C++ compiler, if returns TRUE, you are good to go
pkgbuild::has_build_tools(debug=TRUE)
```
<br/>

## **Background**
**Statistics**: making inferences about an uncertain world using samples of observations (data)

The bedrock of statistics is probability theory. Much early work in probability and statistics were done by Bayesians. In the late 19$^{th}$ and early 20$^{th}$ centuries, statistics was taken over by Fisher and the frequentists. The exponential growth in human computing power and advent of sampling algorithms like MCMC during the 20$^{th}$ made previously intractible problems solvable. Today the debate between these philosophies of statistics is over among mathemeticians and statisticians.

    
The difference between Bayesian and Classical (a.k.a. frequentist) inference is subtle.  

  
### **Freqentist:** 
* Data are random.  
* Parameters ($\theta$) are fixed, but uncertain.

Estimate $\theta$ as the value that maximizes $P(data|\theta)$ (maximum likelihood)

Parameter estimates interpreted in terms of long-run frequency of outcomes in hypothetical repeated experiments.  
  
A 95% _confidence_ interval is interpreted as:  
"if the experiment were repeated a large number of times, the 'true' $\theta$ would fall within this range 95% of the time.
     
### **Bayesian:** 
* Data are fixed. 
* Parameters ($\theta$) are random

Estimate $P(\theta|data)$ using Bayes' rule:
$$ P(\theta|data)\propto \frac{P(data|\theta)P(\theta)}{P(data)}$$


MCMC is the the engine that gets us our answer.  

Parameter estimates are a probability distribution, interpretation is thus very straightforward. Posterior estimates can be interpreted the way most people probably _think_ about traditional confidence intervals.  

A 95% credible interveral would be interpreted as:
"Given these data and this model, there is a 95% chance $\theta$ lies in this range".  

## Statistics now

Whichever school of thought  you choose, inference is moving away from tests of 'significance' and $p$ values, focusing more on making inference about parameter estimates and their uncertainty (i.e., estimates and CI's).  

A very recent [paper](https://www.nature.com/articles/d41586-019-00857-9) in Nature calls for the retirement of $p$ values and suggests rebranding confidence intervals and credible intervals as 'compatibility intervals'. Both 'confidence' and 'credible' are somewhat loaded terms. Inferences made from data and models are only _credible_ (or you can only have _confidence_ in the inferences) if the assumptions you've made in collecting the data and building the model are correct (remembering all models wrong, some useful). Instead, we should consider point and interval estimates from a model to be _compatible_ with the data and assumed model structure.  
  

 
![Yoda's image &copy; Disney](yoda.jpg)  

## Why go Bayesian?  
 
**Reasons for:**  

* Straightforward interpretation of results (direct probability statements about parameters).
* Propagation of uncertainty (compatibility intervals on everything for _FREE_!!).  
* Tests for differences very straightforward, allow you to calculate probability of a difference.
* Approachable modeling notation, which provides a 'common tongue' for modelers to share ideas transparently.

     + JAGS models are written almost entirely in the mathematical notation you would to describe a model in a write-up.
     + Stan is too, but its syntax is a bit more complicated. I've found learning the basics of Bayesian Inference to be easier with JAGS models.

**Caveats**  

* There is a bit more overhead in terms of model setup and obtaining results:  
     + Determining appropriate priors
     + Tuning the model to reach convergence
     + Computation time



## Example 1: Estimating the probability of occurrence a binary outcome.
<br/>

Lets say someone wants to know if a coin they have is fair, or if it is a trick coin from a magic store.
Now before flipping the coin, this person has no idea whether this coin is fair or not, because the friend that gave it to them is known for practical jokes.

Our investigator wants to estimate something about this coin, namely what is the probability of getting a heads on a flip of the coin ($\theta$)?

To use Bayes rule to estimate $\theta$ we'll need some data, a likelihood for those data, and a prior distribution for our parameters:

$$ P(parameters|data)\propto \frac{likelihood \times prior}{P(data)}$$
We don't need to worry about $P(data)$, it is a constant (remember in Bayes' world the data are fixed), so is integrated out.

Because we are omniscient in this story, we know that the coin is a fair coin.

```{r simulate binom data }

# For reproducibility
set.seed(1141)

# Simulate 100 tosses of a fair coin
y <- rbinom(n=100 , size=1, p=0.5)

```

Now we have some data ($y$), next we need a model for $P(data|\theta)$ (the probability of observing these data, given the parameters). This is the 'likelihood'of the data. Here is where we choose a probability distribution that represents how observations $y$ came to be.

In this case, a binomial distribution is a good model of a binary outcome. So we can write the likelihood ($P(data|\theta)$) of the $data$ as: $$y\sim binomial(p, N)$$ , where $p$ is the probability of getting a heads, and $N$ is the number of coin tosses. We'll use the $bernoulli$ distribution in our model, it's just a binomial with N=1.

Now how should we characterize our prior beliefs about $P(\theta)$?  
Lets say our investigator has no belief either way whether this coin is fair or not. Because our parameter $\theta$ is a probability, it has to be between 0 and 1.  But to reflect the investigators lack of knowledge, perhaps they believe _a priori_ that $\theta$ is equally likely to be anywhere between 0 and 1. They might then choose a uniform distribution between 0 and 1 as  $P(\theta)$):
$$\theta\sim uniform(0,1)$$  
Now we have everything we need to define our model.  
  
## Some notes on JAGS  
  
* JAGS takes models as a text string.  
* JAGS is a declarative language, not procedural (like R: do this, then that, etc.). 
* Declarative means describing what to do, not how to do it. It doesnt matter what order you specify your model.  
     + Another example of declarative is SQL (pick a dialect)  
*  Can edit model text in another text editor for help tracking parens and brackets etc.  

```{r binom model}
# JAGS model of our coin-toss experiment
 mod <- 
      "model{

           # Likelihood of the data: P(data|theta)
           for (i in 1:n_tosses){
                y[i] ~ dbern(theta)
           }

           # Prior on theta: P(theta)
           theta ~ dunif(0, 1)
      
     }"
```

We'll use rjags and coda for this example.
```{r run binom example, results='hide',cache=TRUE}
# Prepare data for JAGS:
data <- list(y=y, n_tosses=length(y))
  
# Initialize our JAGS model
jm <- jags.model(textConnection(mod), data=data, n.chains=3)

# Burn-in
update(jm, 1000)

# Run the mcmc sampler
samps <- coda.samples(jm, variable.names="theta", n.iter=100000, thin=4)

```

**Trace plots and posterior distribution of $\theta$**

```{r binomial trace and density plot}
plot(samps)

```
<br/>  

# Linear regression with the Iris data

```{r JAGS lm model}
mod <-"
model{

# For each observation in 1:n  
  for (i in 1:n){

  # Likelihood of the data

     pw[i] ~ dnorm(mu[i], tau)


  # Regression equation

     mu[i] <-  B0 + B1*pl[i]
  
  # Calculate residuals 'by hand'
     resid[i] <-  mu[i] - pw[i]
  } 


  # Priors

        # Intercept
        B0 ~ dnorm(0, 1/1000)

        # Slope
        B1 ~ dnorm(0, 1/1000)


    # Residual standard deviation
        resid.sd ~ dgamma(0.001, 0.001)

    # Transform sd to precision (JAGS uses precision instead of variance in normal distribution)
        tau <- 1 / resid.sd^2

}
"

```
  
We'll use runjags with parallel chains for this example.  
  
```{r prep Iris data}

# Prepare the iris data in a list
data <- list(pw=iris$Petal.Width,
             pl=iris$Petal.Length,
             n=nrow(iris))
```

```{r run JAGS lm, cache=TRUE}
# Run the model
results <- run.jags(model=mod, 
              data=data, 
              n.chains=3,
              adapt=1000,
              burnin=5000,
              monitor=c("B0", "B1", "resid.sd"),
              sample=10000,
              thin=5,
              method="rjparallel")
```

```{r JAGS lm summary}

# Check out summary of model run
summary(results)
```

**Check for convergence**  

```{r JAGS lm trace and density plots}

results$mcmc %>% plot()
```
  
  
**Look for autocorrelation in samples**  

```{r JAGS lm autocorr plot}
# Look for autocorrelation in chains
results$mcmc %>% autocorr.plot()
```
  
## **Compare to classical lm** 

```{r traditional lm}
# MLE fit
m <- lm(Petal.Width ~ Petal.Length, data=iris) %>% summary

# We get the same answer
bayes <- summary(results$mcmc)[[1]][,1:2]

MLE <- rbind(Coefs=m$coefficients[,1:2], Resid.sd=c(m$sigma, NA))

print(list(Bayesian=bayes, Classical=MLE))
```
  
  
<br/>

# Iris regression in Stan

Many code editors (including Rstudio) recognize .stan file extension and help with syntax highlighting 

Here's the model as a text string  
Stan has 5 model blocks

     
```{r stan lm}
mod_txt <- "
data{

// Comments are forwardslash in stan

// Define data and their dimensions

int<lower=0> n; // number of observations
vector[n] pw; // petal width measurements
vector[n] pl; // petal length measurements

}

// Define model parameters

parameters{

real B0; // Intercept
real B1; // Slope of petal length
real<lower=0> resid_sd; // residual standard deviation

}

// Regression equation in the transformed parameters block 

transformed parameters{

vector[n] mu; // Define dimensions of mu, the predicted value at each data point

mu = B0 + B1*pl;


}

model{

// Data likelihood
pw ~ normal(mu, resid_sd); //  pw ~ normal(B0+B1*pl, resid_sd)

// Priors
B0 ~ normal(0, 100); // Vague prior on intercept
B1 ~ normal(0, 100); // Vague prior on petal length effect (slope)

resid_sd ~ gamma(0.001, 0.001); // Vague prior on residual error

}

generated quantities{

vector[n] resid; // Residuals
resid = pw - mu; // Calculate resids 'by hand'

}

"
```

```{r run stan model from text string}
# out <- stan(model_code=mod_txt, data=data, pars=c("B0","B1","resid_sd"))
# summary(out)$s

```

## **Run stan model from stan chunk**  
Using a stan chunk gives you the syntax highlighting in a markdown doc, Nice!
```{stan output.var="mod", cache=TRUE,label="stan chunk"}
data{

// Comments are forwardslash in stan

// Define data and their dimensions

int<lower=0> n; // number of observations
vector[n] pw; // petal width measurements
vector[n] pl; // petal length measurements

}

// Define model parameters

parameters{

real B0; // Intercept
real B1; // Slope of petal length
real<lower=0> resid_sd; // residual standard deviation

}

// Regression equation in the transformed parameters block 

transformed parameters{

vector[n] mu; // Define dimensions of mu, the predicted value at each data point

mu = B0 + B1*pl;


}

model{

// Data likelihood
pw ~ normal(mu, resid_sd); //  pw ~ normal(B0+B1*pl, resid_sd)

// Priors
B0 ~ normal(0, 100); // Vague prior on intercept
B1 ~ normal(0, 100); // Vague prior on petal length effect (slope)

resid_sd ~ gamma(0.001, 0.001); // Vague prior on residual error

}



generated quantities{

vector[n] resid; // Residuals
resid = pw - mu; // Calculate resids 'by hand'

}


```

```{r run stan model from stan chunk,cache=TRUE, comment=NA}
# Using the stan chunk above, need to use sampling() instead of stan()
fit <- sampling(mod,data=data,pars=c("B0","B1","resid_sd") )
summary(fit)$s
```

# Model diagnostics with Shinystan
```{r launch shinystan, eval=FALSE}
shinystan::launch_shinystan(fit)
```


<br/>

# Session Info (for reproducibility)  
<br/>

```{r session info,echo=FALSE}
print(sessionInfo())
```


